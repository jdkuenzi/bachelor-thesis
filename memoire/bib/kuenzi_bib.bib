@online{noauthor_crossentropyloss_nodate,
	title = {{CrossEntropyLoss} — {PyTorch} 1.8.1 documentation},
	shorttitle = {{CrossEntropyLoss}},
	url = {https://pytorch.org/docs/1.8.1/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss},
	urldate = {2021-07-09},
	langid = {english},
	file = {CrossEntropyLoss — PyTorch 1.8.1 documentation:C\:\\Users\\Walid\\Zotero\\storage\\MVJZTPHG\\torch.nn.CrossEntropyLoss.html:text/html},
}

@online{noauthor_batchnorm1d_nodate,
	title = {{BatchNorm}1d — {PyTorch} 1.8.1 documentation},
	url = {https://pytorch.org/docs/1.8.1/generated/torch.nn.BatchNorm1d.html?highlight=batchnorm#torch.nn.BatchNorm1d},
	shorttitle = {{BatchNorm}1d},
	urldate = {2021-07-07},
	langid = {english},
	file = {BatchNorm1d — PyTorch 1.8.1 documentation:C\:\\Users\\Walid\\Zotero\\storage\\5MQDJ98E\\torch.nn.BatchNorm1d.html:text/html},
}

@inreference{noauthor_harmonique_2021,
	title = {Harmonique (musique)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://fr.wikipedia.org/w/index.php?title=Harmonique_(musique)&oldid=180763506},
	shorttitle = {Harmonique},
	abstract = {En acoustique, un partiel harmonique est une composante d’un son périodique, dont la fréquence est un multiple entier d'une fréquence fondamentale.

Si on appelle « ƒ » la fréquence fondamentale, les partiels harmoniques ont des fréquences égales à : 2ƒ, 3ƒ, 4ƒ, 5ƒ, etc.

Les partiels harmoniques sont des composants importants d’un son musical. La fondamentale détermine la hauteur perçue, la puissance relative des harmoniques de rang supérieur influe, avec des caractères dynamiques, sur le timbre.
En musique, par assimilation, on appelle « harmoniques » les sons qu'on obtient sur les instruments à cordes en forçant la vibration d'une corde à un mode supérieur à son mode fondamental. Par exemple, en effleurant la corde au tiers de sa longueur, on empêche son déplacement latéral à cet endroit, tout en la laissant osciller autour de ce point fixe, créant un nœud qui l'oblige à vibrer à une fréquence triple de celle qu'elle aurait, libre. Le son ainsi produit se trouve à un intervalle de douzième avec celui de la corde libre (une octave plus une quinte).
Le mot « harmonique » est utilisé aussi de manière moins technique pour désigner des éléments de l'harmonie, par exemple dans l'expression « intervalle harmonique », qui désigne simplement un intervalle appartenant à l'harmonie.},
	booktitle = {Wikipédia},
	urldate = {2021-05-15},
	date = {2021-03-11},
	langid = {french},
	note = {Page Version {ID}: 180763506},
	file = {Snapshot:C\:\\Users\\Walid\\Zotero\\storage\\E8I6QP3G\\index.html:text/html},
}

@online{noauthor_alphafold_nodate,
	title = {{AlphaFold}: a solution to a 50-year-old grand challenge in biology},
	url = {https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology},
	shorttitle = {{AlphaFold}},
	abstract = {In a major scientific advance, the latest version of our {AI} system {AlphaFold} has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction ({CASP}) assessment. This breakthrough demonstrates the impact {AI} can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world.},
	titleaddon = {Deepmind},
	urldate = {2021-07-20},
	file = {Snapshot:C\:\\Users\\Walid\\Zotero\\storage\\GEAQEZ2B\\alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology.html:text/html},
}

@inreference{noauthor_theoreme_2021,
	title = {Théorème d'échantillonnage},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://fr.wikipedia.org/w/index.php?title=Th%C3%A9or%C3%A8me_d%27%C3%A9chantillonnage&oldid=183959686},
	abstract = {Le théorème d'échantillonnage, dit aussi théorème de Shannon ou théorème de Nyquist-Shannon, établit les conditions qui permettent l'échantillonnage d'un signal de largeur spectrale et d'amplitude limitées.
La connaissance de plus de caractéristiques du signal permet sa description par un nombre inférieur d'échantillons, par un processus d'acquisition comprimée.},
	booktitle = {Wikipédia},
	urldate = {2021-05-14},
	date = {2021-06-20},
	langid = {french},
	note = {Page Version {ID}: 183959686},
	file = {Snapshot:C\:\\Users\\Walid\\Zotero\\storage\\XZFVGBPH\\index.html:text/html},
}


@article{peeters_ensea_2017,
	title = {{ENSEA} 3ème {SyM} (2017-2018)  Traitement du signal audio musical   Partie 1: transformation et séparation du son},
	url = {http://recherche.ircam.fr/anasyn/peeters/pub/cours/20171010_Peeters_20172018_ENSEA_3SYM_Cours_Transformation.pdf},
	shorttitle = {{ENSEA} 3ème {SyM} (2017-2018)  Traitement du signal},
	pages = {79},
	author = {Peeters, Geoffroy},
	urldate = {2021-05-17},
	date = {2017-10-10},
	langid = {french},
	file = {Cours_Transformation.pdf:C\:\\Users\\Walid\\Desktop\\HEPIA\\Projet de semestre\\Cours_Transformation.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki; [et al.]},
	urldate = {2021-07-20},
	date = {2017-12-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {att_is_all_u_need_1706.03762.pdf:C\:\\Users\\Walid\\Desktop\\HEPIA\\memoire\\att_is_all_u_need_1706.03762.pdf:application/pdf},
}

@article{bank_autoencoders_2021,
	title = {Autoencoders},
	url = {http://arxiv.org/abs/2003.05991},
	abstract = {An autoencoder is a speciﬁc type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the diﬀerent types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.},
	journaltitle = {{arXiv}:2003.05991 [cs, stat]},
	author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
	urldate = {2021-06-15},
	date = {2021-04-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.05991},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {autoencoder_2003.05991.pdf:C\:\\Users\\Walid\\Desktop\\HEPIA\\memoire\\autoencoder_2003.05991.pdf:application/pdf},
}

@article{ioffe_batch_2015,
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	url = {http://arxiv.org/abs/1502.03167},
	shorttitle = {Batch Normalization},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classiﬁcation model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a signiﬁcant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on {ImageNet} classiﬁcation: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	journaltitle = {{arXiv}:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	urldate = {2021-07-08},
	date = {2015-03-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {batch_norm_1502.03167.pdf:C\:\\Users\\Walid\\Desktop\\HEPIA\\memoire\\batch_norm_1502.03167.pdf:application/pdf},
}

@article{santurkar_how_2019,
	title = {How Does Batch Normalization Help Optimization?},
	url = {http://arxiv.org/abs/1805.11604},
	abstract = {Batch Normalization ({BatchNorm}) is a widely adopted technique that enables faster and more stable training of deep neural networks ({DNNs}). Despite its pervasiveness, the exact reasons for {BatchNorm}’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of {BatchNorm}. Instead, we uncover a more fundamental impact of {BatchNorm} on the training process: it makes the optimization landscape signiﬁcantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
	journaltitle = {{arXiv}:1805.11604 [cs, stat]},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
	urldate = {2021-07-07},
	date = {2019-04-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.11604},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {how_batchnorm_1805.11604.pdf:C\:\\Users\\Walid\\Desktop\\HEPIA\\memoire\\how_batchnorm_1805.11604.pdf:application/pdf},
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: A Generative Model for Raw Audio},
	url = {http://arxiv.org/abs/1609.03499},
	shorttitle = {{WaveNet}},
	abstract = {This paper introduces {WaveNet}, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efﬁciently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as signiﬁcantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single {WaveNet} can capture the characteristics of many different speakers with equal ﬁdelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we ﬁnd that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	journaltitle = {{arXiv}:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga; [et al.]},
	urldate = {2021-07-21},
	date = {2016-09-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {wave_net_1609.03499.pdf:C\:\\Users\\Walid\\Desktop\\HEPIA\\memoire\\wave_net_1609.03499.pdf:application/pdf},
}

@article{baevski_wav2vec_2020,
	title = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
	url = {http://arxiv.org/abs/2006.11477},
	shorttitle = {wav2vec 2.0},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 {WER} on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 {WER}. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	journaltitle = {{arXiv}:2006.11477 [cs, eess]},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	urldate = {2021-07-21},
	date = {2020-10-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.11477},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {wave2vec2_0.2006.11477.pdf:C\:\\Users\\Walid\\Desktop\\HEPIA\\memoire\\wave2vec2_0.2006.11477.pdf:application/pdf},
}