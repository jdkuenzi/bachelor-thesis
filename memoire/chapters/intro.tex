% !TeX spellcheck = fr_FR
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction} % Adding toc entry

L’\gls{ia} a révolutionné bien des domaines lors des décennies passées. Même en musique, l’\gls{ia} est désormais omniprésente, allant du simple filtre à la composition de morceau\footnote{\url{https://www.aiva.ai/}}. Généralement, ses applications sont définies pour le monde professionnel, mais avec l'explosion des technologies de l'information et l'accessibilité des données, beaucoup de personnes s'intéressent au monde de la musique de manière autodidacte. Étant moi-même de base autodidacte, j'ai beaucoup appris en écoutant des morceaux et en essayant de reproduire les accords. Toutefois, je me rendais compte que même si j'arrivais à reproduire les accords à l'oreille, je ne connaissais ni leurs noms ni leurs formes (majeure, mineure, etc.). En discutant avec mon prof de musique, il m'a fait part de la même constatation chez certains de ses élèves. C'est pourquoi j'ai eu l'idée de faire une \gls{ia} capable de reconnaitre des notes et accords de guitare, ainsi, il est possible pour les personnes autodidactes d'avoir plus de facilité à comprendre la structure d'un morceau ou d'une gamme.

Ce travail est la continuité de mon travail précédent, dont le but était de détecter des notes de trompette sur une octave de Do. L'objectif de cette nouvelle étude est d'utiliser différentes architectures de modèle connexionniste (réseau neuronal) et de voir s'il est possible de détecter des notes et accords de guitare pour des sons mono échantillonnés à 44.1[kHz]. De plus il sera possible de considérer le signal dans le monde discret (échantillonné) et dans le monde fréquentiel. L’objectif est ainsi d’arriver à une utilisation en temps réel avec le moins de latence possible (visuelle et auditive). Dans le monde de la musique, une latence supérieure à cinq millisecondes pour le traitement du signal (calculs) est indésirable et se fait fortement ressentir. Le challenge est donc que les architectures proposées prédisent le plus précisément possible l'accord ou la note jouée et tout ça dans un temps inférieur à cinq millisecondes. Bien sûr, comme il n'est pas possible de faire du temps réel en informatique, l'objectif sera d'utiliser un temps d'échantillonnage suffisamment court (moins d'un dixième de seconde) afin de ne pas ressentir le temps de latence.

Pour ce travail, j'ai dû également créer un ensemble de données en partant de zéro. Par rapport au temps et aux ressources mises à ma disposition, j'ai décidé de m'attaquer à la gamme tempérée de la musique occidentale (pas de musique microtonale) et de représenter les accords mineurs et majeurs à trois sons dans l'accordage standard d'une guitare (Mi, La, Ré, Sol, Si, Mi). Ces accords seront joués jusqu'à la $12^{\text{ème}}$ frette (case de la guitare) non incluse et sur les deux cordes les plus graves, à savoir Mi et La. Quant aux notes, elles sont toutes représentées dans leurs différentes octaves et positions, toujours par rapport à l'accordage standard.

Les premières semaines de travail m'ont servi à récolter un maximum d'informations sur les solutions similaires existantes (traitement de la voix, détection de clés, etc.) et à comprendre les notions mathématiques nécessaires à la réalisation du travail. Pour les notions mathématiques je me suis basé sur un cours de $3^{\text{ème}}$ année à l'École Nationale Supérieure de l'Électronique et de ses Applications (ENSEA) sur le traitement de signal \parencite{peeters_ensea_2017} présenté par le professeur Geoffroy \textsc{Peeters}. Il est chercheur à l’Institut de Recherche et Coordination Acoustique/Musique (IRCAM) où il supervise notamment les travaux de Master et de Doctorat.

Durant ces premières semaines, j'ai aussi profité de mettre en place un environnement virtuel Conda\footnote{\url{https://conda.io/projects/conda/en/latest/index.html}} afin de bénéficier d'un environnement de travail dans lequel je pourrais utiliser les API d'apprentissage profond.

Étant familier avec l'API d'apprentissage profond Keras\footnote{\url{https://keras.io/}} (API de Google), j'ai commencé à développer de petits réseaux de neurones afin de bien comprendre comment fonctionnent les cellules LSTM que je n'avais jusqu'alors pas réellement explorées. Toutefois, Keras est une API assez haut niveau qui restreint un peu l'utilisateur, c'est pourquoi j'ai décidé de partir sur PyTorch\footnote{\url{https://pytorch.org/}} (API de Facebook) qui est plus bas niveau, donc plus difficile à utiliser, mais qui me permettait d'être plus libre, surtout sur la création de couches personnalisées et les architectures de mes modèles. J'ai donc dû lire la documentation PyTorch et dû effectuer quelques expérimentations pour apprendre à utiliser correctement cette nouvelle API.

Durant mon apprentissage de l'API PyTorch, j'ai aussi pu examiner les modèles proposés par la communauté PyTorch et notamment ceux qui sont en lien avec le monde de la musique comme OPEN-UNMIX\footnote{\url{https://github.com/sigsep/open-unmix-pytorch}}. Ces modèles sont généralement bien documentés et m'ont permis d'acquérir et de comprendre des nouvelles bases comme les skip-layer, la normalisation par lots ou encore les cellules LSTM bidirectionnelles. Pour ces nouvelles connaissances, je me suis basé sur des études pour la plupart disponibles sur la plateforme arXiv\footnote{\url{https://arxiv.org/}} (plateforme d'archives de prépublication scientifique) qui m'ont permis de comprendre l'essentiel des informations. Elles ont aussi servi à renforcer mes bases déjà acquises et à me rendre plus confiant quant à mes choix.

Je suis ensuite passé à une phase d'expérimentation pendant laquelle j'ai testé plusieurs types d'architectures pour mes modèles connexionnistes, différentes tailles de fenêtres (temps d'échantillonnage) et différentes techniques de traitement du signal. J'ai notamment aussi enregistré deux ensembles de données, l'un avec une carte son interne à mon ordinateur et l'autre avec une carte son externe spécialisée dans le traitement audio.

Afin de sauvegarder mon travail, j'ai créé un répertoire Git\footnote{\url{https://gitedu.hesge.ch/jeandani.kuenzi/bachelor.git}} sur lequel se trouve mon ensemble de données, ma thèse au format PDF ainsi qu'au format LaTeX, les images présentes dans cette thèse, le code ayant servi à entrainer et créer mes différentes architectures ainsi que mon environnement Conda au format YAML.

Au cours de cette thèse, nous verrons tout d'abord les solutions existantes que j'ai retenues et nous les détaillerons un peu.

Puis, nous verrons les notions théoriques nécessaires à la bonne compréhension du travail. Il y aura tout d'abord les notions musicales qui expliqueront ce que c'est qu'un son, comment est composé un accord et comment différencier un accord majeur d'un accord mineur.

Ensuite, il y aura une partie sur les notions mathématiques afin de bien comprendre comment on représente un son dans le monde fréquentiel et comment on passe du monde discret au monde fréquentiel ainsi que les différents problèmes qui peuvent y survenir.

Nous verrons également les notions nécessaires à la bonne compréhension de ce qu'est un réseau de neurones et comment il fonctionne depuis sa base qui est le Perceptron.

Je parlerai également de la manière dont j'ai créé mon ensemble de données, la structure des fichiers audio que j'ai enregistrés, le matériel utilisé et les différentes techniques que j'utilise dans le prétraitement de celui-ci.

Ensuite, j'expliquerai ma solution ainsi que les différentes parties qui la composent. Je vais notamment dans cette partie justifier mon choix concernant l'architecture de cette dernière.

Après cela, je présenterai sans rentrer dans le détail les architectures que j'ai testées durant mes expérimentations et que je n'ai pas retenues.

Finalement, je présenterai et débattrai des différents résultats obtenus avec ma solution et j'expliquerai également mon ressentiment d'utilisation en temps réel.